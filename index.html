<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>TemCap – Project Page</title>
  <link rel="icon" type="image/png" href="./assets/images/Browser_Icon.png" />
  <!-- Tailwind (via CDN) -->
  <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-white text-gray-800">

  <!-- ==========  MOBILE‑ONLY COLLAPSIBLE MENU  ========== -->
  <details class="md:hidden bg-white border-b border-gray-200 shadow-sm mb-2">
    <summary class="px-4 py-3 font-semibold cursor-pointer select-none">
      Menu
    </summary>
    <div class="px-4 pb-5 text-sm space-y-6 leading-relaxed">
      <nav class="space-y-1">
        <a href="#abstract"   class="block hover:underline text-gray-700">Abstract</a>
        <a href="#method"     class="block hover:underline text-gray-700">Method</a>
        <a href="#contributions"    class="block hover:underline text-gray-700">Contributions</a>
        <a href="#metrics"    class="block hover:underline text-gray-700">Metrics</a>
        <a href="#impact"    class="block hover:underline text-gray-700">Impact</a>
        <a href="#resources"  class="block hover:underline text-gray-700">Resources</a>
        <a href="#acks"       class="block hover:underline text-gray-700">Acknowledgments</a>
        <a href="#citation"   class="block hover:underline text-gray-700">Citation</a>
        <a href="#contact"    class="block hover:underline text-gray-700">Contact</a>
      </nav>
      <hr>
      <!-- Paper Links -->
      <div>
        <h3 class="font-semibold mb-1">Paper Links</h3>
        <ul class="list-disc list-inside space-y-1">
          <li><a href="https://arxiv.org/pdf/2505.16594" class="text-blue-600 hover:underline">PDF (5.35 MB)</a></li>
          <li><a href="https://arxiv.org/abs/2505.16594" class="text-blue-600 hover:underline">arXiv page</a></li>
        </ul>
      </div>
    </div>
  </details>

  <!-- ==========  PAGE WRAPPER  ========== -->
  <div class="flex h-screen overflow-hidden">

    <!-- SIDEBAR — visible ≥ md only -->
    <aside class="hidden md:sticky md:top-0 md:block md:w-60 md:flex-shrink-0 bg-white border-r border-gray-200 p-6 overflow-y-auto">
      <h2 class="text-xl font-bold mb-4">Navigation</h2>
      <nav class="space-y-2 mb-6 text-sm leading-relaxed">
        <a href="#abstract"   class="block hover:underline text-gray-700">Abstract</a>
        <a href="#method"     class="block hover:underline text-gray-700">Method</a>
        <a href="#contributions"    class="block hover:underline text-gray-700">Contributions</a>
        <a href="#metrics"    class="block hover:underline text-gray-700">Metrics</a>
        <a href="#impact"    class="block hover:underline text-gray-700">Impact</a>
        <a href="#resources"  class="block hover:underline text-gray-700">Resources</a>
        <a href="#acks"       class="block hover:underline text-gray-700">Acknowledgments</a>
        <a href="#citation"   class="block hover:underline text-gray-700">Citation</a>
        <a href="#contact"    class="block hover:underline text-gray-700">Contact</a>
      </nav>

      <hr class="my-6">
      <h3 class="font-semibold mb-2 text-sm">Paper Links</h3>
      <ul class="list-disc list-inside space-y-1 text-sm leading-relaxed mb-6">
        <li><a href="https://arxiv.org/pdf/2505.16594" class="text-blue-600 hover:underline">PDF (5.35 MB)</a></li>
        <li><a href="https://arxiv.org/abs/2505.16594" class="text-blue-600 hover:underline">arXiv page</a></li>
      </ul>
      <hr class="my-6">
    </aside>

    <!-- MAIN CONTENT -->
    <main class="flex-1 overflow-y-auto p-6 md:p-8 space-y-20" id="main">

      <!-- HERO -->
      <header class="text-center mb-12">
        <h1 class="text-4xl font-extrabold leading-tight">
          Temporal Object Captioning for Street Scene Videos from LiDAR Tracks
        </h1>

        <!-- AUTHORS -->
        <p class="text-sm text-gray-500 mt-3">
          <a href="https://osvia.github.io/team.html"
            class="text-blue-600 hover:underline" target="_blank"
            rel="noopener noreferrer">Vignesh Gopinathan</a><sup>1,2</sup> •
          <a href="mailto:urs.zimmermann[at]aptiv.com"
            class="text-blue-600 hover:underline" target="_blank"
            rel="noopener noreferrer">Urs Zimmermann</a><sup>2</sup> •
          <a href="mailto:michael.arnold[at]aptiv.com"
            class="text-blue-600 hover:underline" target="_blank"
            rel="noopener noreferrer">Michael Arnold</a><sup>2</sup> •
          <a href="https://osvia.github.io/team.html"
            class="text-blue-600 hover:underline" target="_blank"
            rel="noopener noreferrer">Matthias Rottmann</a><sup>3</sup>
        </p>

        <!-- AFFILIATION FOOTNOTE -->
        <p class="text-xs text-gray-500 mt-1">
          <sup>1</sup> Department&nbsp;of&nbsp;Mathematics, University&nbsp;of&nbsp;Wuppertal, Wuppertal, Germany
          <br>
          <sup>2</sup> Aptiv, Germany
          <br>
          <sup>3</sup> Institute of Computer Science, Osnabrück University, Osnabrück, Germany
          <br><br>
          Winter Conference on Applications of Computer Vision (WACV), 2026
        </p>

        <!-- LOGO STRIP -->
        <div class="flex flex-wrap justify-center items-center gap-6 mt-4">
          <!-- OSVIA Lab Logo -->
          <a href="https://osvia.github.io/" target="_blank" rel="noopener noreferrer" class="block h-10">
            <img src="./assets/images/KeyVisual_Dark.png" alt="OSVIA Lab Logo" class="h-full w-auto" loading="lazy">
          </a>

          <!-- University of Wuppertal Logo (example) -->
          <a href="https://www.uni-wuppertal.de/" target="_blank" rel="noopener noreferrer" class="block h-10">
            <img src="./assets/images/BUW_Logo.jpg" alt="University of Wuppertal Logo" class="h-full w-auto" loading="lazy">
          </a>

          <!-- University of Osnabrück -->
          <a href="https://www.uni-osnabrueck.de/en" target="_blank" rel="noopener noreferrer" class="block h-10">
            <img src="./assets/images/logo_osnabrück.jpg" alt="University of Osnabrück" class="h-full w-auto" loading="lazy">
          </a>

          <!-- Aptiv -->
          <a href="https://www.aptiv.com/" target="_blank" rel="noopener noreferrer" class="block h-10">
            <img src="./assets/images/aptiv_logo_color_rgb.png" alt="Aptiv" class="h-full w-auto" loading="lazy">
          </a>

          <!-- Add additional logos here (duplicate & adapt) -->
        </div>

      
        <!-- ACTION BUTTONS -->
        <div class="flex flex-wrap justify-center gap-3 mt-6">

          <!-- Paper / Document icon -->
          <a href="https://arxiv.org/abs/2505.16594"
            class="inline-flex items-center px-6 py-3 rounded-xl bg-blue-600 text-white font-semibold hover:bg-blue-700">
            <!-- Heroicons outline: DocumentText -->
            <svg class="w-5 h-5 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor"
                stroke-width="1.5" aria-hidden="true">
              <path stroke-linecap="round" stroke-linejoin="round"
                    d="M6 2.25h7.5L18 6.75v14.25a.75.75 0 01-.75.75H6.75A.75.75 0 016 21V2.25z" />
              <path stroke-linecap="round" stroke-linejoin="round"
                    d="M14.25 2.25v4.5h4.5" />
              <path stroke-linecap="round" stroke-linejoin="round"
                    d="M8.25 9.75h7.5m-7.5 3h7.5m-7.5 3h4.5" />
            </svg>
            arXiv Page
          </a>
        </div>


        <!-- Title Images Row (unchanged) -->
        <div class="flex flex-wrap justify-center gap-2 mt-6">
          <img src="./assets/images/figure1.png"  alt="Title image 1"
              class="h-64 w-auto max-w-[100%] md:max-w-none rounded-lg shadow-sm">
        </div>
      </header>
      <!-- /HERO -->


      <!-- ABSTRACT -->
      <section id="abstract" class="space-y-4 lg:max-w-6xl  lg:mx-auto">
        <h2 class="text-2xl font-semibold">Abstract</h2>
        <p>
          We introduce a fully automated LiDAR‑based captioning pipeline that produces object‑level,
      temporally grounded descriptions of street‑scene videos. The captions encode lane position,
      motion relative to the ego vehicle, and temporal transitions (e.g., lane changes), enabling
      small, efficient video captioners (SwinBERT) trained on front‑camera frames alone to learn
      richer temporal semantics. Across three datasets (proprietary, Waymo, NuScenes), models
      trained with our captions show stronger temporal understanding and markedly reduced reliance
      on static visual backgrounds, as quantified by our Visual Bias Measure (VBM).
        </p>
      </section>

      <!-- METHOD OVERVIEW -->
      <section id="method" class="space-y-6 bg-gray-100 rounded-xl px-6 py-8 lg:max-w-6xl lg:mx-auto">
        <h2 class="text-2xl font-semibold">Method Overview</h2>
        <p>
          Cross‑modality supervision: generate captions from LiDAR, train only on RGB front camera video recording.
        </p>
        <div class="grid md:grid-cols-1 gap-x-6 gap-y-8">
          <!-- Column 1: nd‑to‑end data → caption → training pipeline -->
          <div class="space-y-3">
            <h3 class="text-xl font-semibold mb-3 text-center">End‑to‑end data → caption → training pipeline</h3>
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/method_image0.png" 
                  alt="method_image0" 
                  loading="lazy" 
                  class="w-full h-auto object-cover">
            </figure>
          </div>

          <!-- Column 2: LiDAR‑to‑mask projection (convex hull) -->
          <div class="space-y-3">
            <h3 class="text-xl font-semibold mb-3 text-center">LiDAR‑to‑mask projection (convex hull)</h3>
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/method_image1.png" 
                  alt="method_image1" 
                  loading="lazy" 
                  class="w-full h-auto object-cover">
            </figure>
          </div>
        </div>
        <!-- Additional text -->
          <p>
            <h3 class="text-l font-semibold mb-3">Step 1: Host segmentation.</h3>
            Split videos into action‑consistent segments using ego speed and yaw‑rate (motion & direction tags).
            <h3 class="text-l font-semibold mb-3">Step 2: Neighbor tracking.</h3>
            Detect & track moving agents in LiDAR; restrict to a forward neighborhood visible in the front camera.
            <h3 class="text-l font-semibold mb-3">Step 3: Tagging.</h3>
            Assign per‑timestamp lane (left/right/host/oncoming/left‑lateral/right‑lateral) and motion (approach/away/constant/stationary) from relative pose & distance changes.
            <h3 class="text-l font-semibold mb-3">Step 4: Template captions.</h3>
            Compress tag time‑series into unified tag sequences and render multi‑sentence captions describing initial state and subsequent transitions.
            <h3 class="text-l font-semibold mb-3">Step 5 (optional): Object masks.</h3>
            Project object LiDAR points to the image, build a tight convex‑hull mask, and overlay to focus learning on the captioned agent.
            <h3 class="text-l font-semibold mb-3">Train: SwinBERT on RGB only.</h3>
            Use the captions for masked‑language training on uniformly sampled frames from front‑camera clips.
            
          </p>
      </section>

      <!-- Key Contributions and Results -->
      <section id="contributions" class="space-y-6 bg-gray-100 rounded-xl px-6 py-8 lg:max-w-6xl lg:mx-auto">
        <h2 class="text-2xl font-semibold">Key Contributions and Results</h2>
        <p>
          <b>• Scalable, generic captioning from LiDAR</b> for temporally rich, object‑level descriptions without human labels.
          <br>
          <b>• Temporal understanding beats strong baselines</b> on captioning and retrieval across seen and unseen datasets.
          <br>
          <b>• Visual Bias Measure (VBM):</b> a simple BLEU‑based metric that quantifies reliance on static visual similarity 
          <br>⠀|-> training with our captions <b>consistently lowers VBM</b>. 
          <br>
          <b>• Mask‑augmented training</b> further improves generalization by directing attention to the correct object of interest.

        </p>
      </section>
      
      <!-- METRICS -->
      
      <section id="metrics" class="space-y-6 rounded-xl px-6 py-8 lg:max-w-6xl lg:mx-auto">
          <h2 class="text-2xl font-semibold">Metrics at a Glance</h2>
          <!-- ──  Captioning Quality ───────────────────────────────────── -->
          <div
          class="flex flex-col min-[1360px]:flex-row min-[1360px]:items-start min-[1360px]:gap-8">
          <!-- Explanatory text -->
          <div class="space-y-3 mb-4 min-[1360px]:mb-0 min-[1360px]:w-1/3">
              <h3 class="font-semibold text-lg">
                Captioning Quality (BLEU4 / CIDEr / SPICE)
              </h3>
              <p class="text-sm leading-relaxed">
                Trained on a proprietary dataset with single‑sentence captions, the model generalizes to Waymo and NuScenes,
                outperforming InternVideo, ViCLIP, and CLIP—even when those baselines only choose from candidate captions.
              </p>
          </div>
      
          <!-- Chart -->
          <div class="flex-1 overflow-x-auto">
              <img src="./assets/images/table1.png" alt="table1" class="chart-image w-full border rounded-md" />
          </div>
          </div>
      
        <!-- ── Visual Bias Measure (VBM) ─────────────────────────────────── -->
          <div
              class="flex flex-col min-[1360px]:flex-row min-[1360px]:items-start min-[1360px]:gap-8">
              <!-- Explanatory text -->
              <div
                  class="space-y-3 mb-4 min-[1360px]:mb-0 min-[1360px]:w-1/3">
                  <h3 class="font-semibold text-lg">
                    Visual Bias Measure (VBM)
                  </h3>
                  <p class="text-sm leading-relaxed">
                    BM measures the % drop in retrieval BLEU4 when same‑time of recording/same neighbors are excluded.
                    Lower is better (less reliance on static appearance). Our training yields the <b>lowest VBM</b> across
                    datasets, with mask augmentation reducing VBM by ~<b>5 pp</b> on Waymo and <b>2 pp</b> on NuScenes.
                  </p>
              </div>

              <!-- Chart -->
              <div class="flex-1 overflow-x-auto">
                  <img src="./assets/images/table2.png" alt="table2" class="chart-image w-full border rounded-md"/>
              </div>
          </div>

        <!-- ── Zero‑Shot Generalization to Longer Captions ─────────────────────────────────── -->
        <div
        class="flex flex-col min-[1360px]:flex-row min-[1360px]:items-start min-[1360px]:gap-8">
        <!-- Explanatory text -->
        <div
            class="space-y-3 mb-4 min-[1360px]:mb-0 min-[1360px]:w-1/3">
            <h3 class="font-semibold text-lg">
              Zero‑Shot Generalization to Longer Captions
            </h3>
            <p class="text-sm leading-relaxed">
              Despite training only on single‑sentence captions, our embeddings retrieve videos for <b>two‑sentence</b>
          captions (more complex maneuvers) with a ~<b>20% higher</b> mean BLEU4 than the best baseline (violin‑plot analysis).
            </p>
        </div>

        <!-- Chart -->
        <div class="flex-1 overflow-x-auto">
            <img src="./assets/images/table3.png" alt="table3" class="chart-image w-full border rounded-md"/>
        </div>
    </div>

      </section>
  
  

      <!-- Additional images -->
      <section id="images" class="space-y-6 bg-gray-100 rounded-xl px-6 py-8 lg:max-w-6xl lg:mx-auto">

        <!-- ── IMAGE GRID ───────────────────────────────────────── -->
        <div class="grid grid-cols-2 gap-3 md:gap-4 place-items-center">

          <!-- ── ROW 1 — Segmentation GTs ── -->
          <figure class="text-center w-full">
            <img src="./assets/images/additional_image_0.png" id="gt-orig"
              alt="additional_image_0"
              class="rounded-lg shadow-sm max-h-72 w-full object-contain">
          </figure>

          <figure class="text-center w-full">
            <img src="./assets/images/additional_image_1.png" id="gt-eed"
              alt="additional_image_1"
              class="rounded-lg shadow-sm max-h-72 w-full object-contain">
          </figure>

          <!-- ── ROW 2 — Input Images ── -->
          <figure class="text-center w-full">
            <img src="./assets/images/additional_image_2.png" id="img-orig"
              alt="additional_image_2"
              class="rounded-lg shadow-sm max-h-72 w-full object-contain">
          </figure>

          <figure class="text-center w-full">
            <img src="./assets/images/additional_image_3.png" id="img-eed"
              alt="additional_image_3"
              class="rounded-lg shadow-sm max-h-72 w-full object-contain">
          </figure>
        </div>
      </section>

      <!-- Broader Impact & Potential -->
      <section id="impact" class="space-y-4 lg:max-w-6xl  lg:mx-auto">
        <h2 class="text-2xl font-semibold">Broader Impact & Potential</h2>
        <p>
          • <b>Scaling data generation:</b> The trained video captioning model can be used to generate temporal object captions for camera-only dataset. Additionally, any LiDAR‑equipped dataset can be converted into rich temporal captioning dataset at low cost using out pseudo-labeling method.  
          <br>
          • <b>Beyond captioning:</b> The learned embeddings improve <b>video retrieval</b> by maneuvers and can aid <b>trajectory forecasting, risk reasoning, or behavior search</b> tools. 
          <br>
          • <b>Temporal understanding metric:</b> VBM, a metric to quantitaively evaluate temporal understanding capabilities of models. Lower VBM suggests robustness to background changes—useful for being agnostic to geographic and seasonal shifts.
        </p>
      </section>

      <!-- RESOURCES -->
      <section id="resources" class="space-y-6 px-6 py-8 lg:max-w-6xl lg:mx-auto">
        <h2 class="text-2xl font-semibold">Resources & Links</h2>
        <ul class="list-disc list-inside space-y-2">
          <li><a href="https://arxiv.org/pdf/2505.16594" class="text-blue-600 hover:underline">PDF (5.35 MB)</a></li>
          <li><a href="https://arxiv.org/abs/2505.16594" class="text-blue-600 hover:underline">arXiv abstract &amp; BibTeX</a></li>
          <li><a href="#" class="text-blue-600 hover:underline">Code (coming soon)</a></li>
        </ul>
      </section>

      <!-- ACKNOWLEDGMENTS -->
      <section id="acks" class="space-y-6 px-6 py-8 lg:max-w-6xl lg:mx-auto">
        <h2 class="text-2xl font-semibold">Acknowledgments</h2>
        <div class="mt-4 w-40 flex flex-col items-center">
          <a href="https://osvia.github.io/" target="_blank" rel="noopener noreferrer">
            <img src="./assets/images/KeyVisual_Dark.png" alt="OSVIA Lab Logo" loading="lazy" class="w-full h-auto">
          </a>
          <p class="mt-2 text-sm text-gray-600 text-center">
            <a href="https://osvia.github.io/" class="text-blue-600 hover:underline" target="_blank" rel="noopener noreferrer">OSVIA Lab</a>
          </p>
        </div>
      </section>

      <!-- CITATION (bottom) -->
      <section id="citation" class="space-y-6 px-6 py-8 lg:max-w-6xl lg:mx-auto">
        <h2 class="text-2xl font-semibold">Citation</h2>

        <div class="relative group">
          <!-- BibTeX code block -->
          <pre id="bibtex-bottom"
                class="whitespace-pre overflow-x-auto
                      bg-gray-100 rounded-lg p-3
                      text-[0.70rem] leading-5 font-mono
                      text-gray-700 select-all">
                      @inproceedings{temporal-object-captioning-wacv2026,
                        title={Temporal Object Captioning for Street Scene Videos from LiDAR Tracks},
                        author={To be updated post-review},
                        booktitle={WACV},
                        year={2026},
                        note={Under review; project page}}
          </pre>

          <!-- copy‑to‑clipboard button -->
          <button
            type="button"
            aria-label="Copy BibTeX"
            class="absolute top-2 right-2
                    flex items-center gap-1
                    px-2 py-1 rounded-md text-xs font-medium
                    bg-gray-200 hover:bg-gray-300
                    transition
                    group-hover:opacity-100 opacity-0"
            onclick="
              navigator.clipboard
                .writeText(document.getElementById('bibtex-bottom').innerText)
                .then(btn=>{
                  this.textContent='✔︎ Copied';
                  setTimeout(()=>this.textContent='Copy',1500);
                });
            ">
            Copy
          </button>
        </div>
      </section>
      <!-- /CITATION (bottom) -->
      

      <!-- CONTACT -->
      <section id="contact" class="space-y-6 px-6 py-8 lg:max-w-6xl lg:mx-auto">
        <h2 class="text-2xl font-semibold">Contact</h2>
        <p class="text-sm">Have questions or want to collaborate? Reach out:</p>
        <ul class="space-y-1 text-sm">
          <li>Email: <a href="mailto:gopinathan[at]uni-wuppertal.de" class="text-blue-600 hover:underline">gopinathan[at]uni-wuppertal.de</a></li>
        </ul>
      </section>

      <!-- FOOTER -->
      <footer class="pt-12 pb-6 text-center text-xs text-gray-500">
        © 2025 From Label Error Detection to Correction • MIT License • Built using the 
        <a href="https://github.com/mmistakes/minimal-mistakes" target="_blank" class="text-blue-600 hover:underline">
          Minimal Mistakes layout template
        </a> • Site built by
        <a href="https://osvia.github.io/team.html" target="_blank" class="text-blue-600 hover:underline">
          Marco Schumacher
        </a>
      </footer>

    </main>
  </div>
  <!-- ==========  /PAGE WRAPPER  ========== -->
</body>
</html>